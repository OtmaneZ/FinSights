import type { NextApiRequest, NextApiResponse } from 'next'
import OpenAI from 'openai'
import { SYSTEM_PROMPT, buildFinancialContext } from '@/lib/copilot/prompts'
import { storeConversation, searchSimilarConversations } from '@/lib/vectordb/collections'

interface CopilotRequest {
    message: string
    rawData?: any[]
    companyName?: string
    conversationHistory?: Array<{
        role: 'user' | 'assistant'
        content: string
    }>
}

interface CopilotResponse {
    success: boolean
    response?: string
    error?: string
}

export default async function handler(
    req: NextApiRequest,
    res: NextApiResponse<CopilotResponse>
) {
    if (req.method !== 'POST') {
        return res.status(405).json({
            success: false,
            error: 'M√©thode non autoris√©e'
        })
    }

    try {
        const { message, rawData, companyName, conversationHistory }: CopilotRequest = req.body

        if (!message || typeof message !== 'string' || message.trim().length === 0) {
            return res.status(400).json({
                success: false,
                error: 'Message requis'
            })
        }

        console.log('ü§ñ Copilot v2.0 - Requ√™te:', {
            message: message.substring(0, 100),
            hasData: !!rawData,
            dataCount: rawData?.length || 0,
            company: companyName
        })

        // üß† Rechercher conversations similaires dans Pinecone
        let contextFromMemory = '';
        if (process.env.PINECONE_API_KEY) {
            try {
                const similarConvs = await searchSimilarConversations(message, companyName, 3);
                if (similarConvs.length > 0) {
                    contextFromMemory = '\n\nüí≠ M√©moire (conversations similaires pass√©es):\n' +
                        similarConvs.map((conv, i) =>
                            `${i + 1}. ${conv.metadata.message} ‚Üí ${conv.metadata.response.substring(0, 100)}...`
                        ).join('\n');
                    console.log(`üß† ${similarConvs.length} conversations similaires trouv√©es`);
                }
            } catch (err) {
                console.warn('‚ö†Ô∏è Erreur m√©moire vectorielle (non-bloquant):', err);
            }
        }

        // Pas de cl√© API ? Mode d√©mo
        if (!process.env.OPENAI_API_KEY) {
            console.warn('‚ö†Ô∏è OPENAI_API_KEY manquante - Mode d√©mo')
            return res.status(200).json({
                success: true,
                response: `ü§ñ **Mode D√©mo** (cl√© OpenAI manquante)

Votre question : "${message}"

Pour activer l'IA compl√®te, ajoutez votre cl√© OpenAI dans \`.env.local\`:
\`\`\`
OPENAI_API_KEY=sk-...
\`\`\`

En attendant, voici ce que je peux dire sur vos donn√©es :
${rawData ? buildFinancialContext(rawData).substring(0, 500) + '...' : 'Aucune donn√©e charg√©e'}`
            })
        }

        // Initialiser OpenAI
        const openai = new OpenAI({
            apiKey: process.env.OPENAI_API_KEY
        })

        // Construire les messages pour GPT
        const messages: OpenAI.Chat.ChatCompletionMessageParam[] = [
            {
                role: 'system',
                content: SYSTEM_PROMPT
            }
        ]

        // Ajouter contexte financier si donn√©es disponibles
        if (rawData && rawData.length > 0) {
            messages.push({
                role: 'system',
                content: buildFinancialContext(rawData) + contextFromMemory
            })
        } else if (contextFromMemory) {
            messages.push({
                role: 'system',
                content: contextFromMemory
            })
        }

        // Ajouter historique conversation (max 5 derniers messages)
        if (conversationHistory && conversationHistory.length > 0) {
            const recentHistory = conversationHistory.slice(-5)
            messages.push(...recentHistory)
        }

        // Ajouter question utilisateur
        messages.push({
            role: 'user',
            content: message
        })

        console.log('üß† Appel OpenAI GPT-4o-mini...')

        // Appel OpenAI
        const completion = await openai.chat.completions.create({
            model: 'gpt-4o-mini',
            messages,
            temperature: 0.3, // Pr√©cis, pas cr√©atif
            max_tokens: 600,
            top_p: 1,
            frequency_penalty: 0,
            presence_penalty: 0
        })

        const response = completion.choices[0].message.content || 'D√©sol√©, je n\'ai pas pu g√©n√©rer de r√©ponse.'

        console.log('‚úÖ R√©ponse g√©n√©r√©e:', response.substring(0, 100) + '...')

        // üíæ Stocker la conversation dans Pinecone (async, non-bloquant)
        if (process.env.PINECONE_API_KEY && companyName) {
            storeConversation(
                companyName, // userId = companyName pour demo
                companyName,
                message,
                response
            ).catch(err => console.warn('‚ö†Ô∏è Erreur stockage conversation (non-bloquant):', err));
        }

        return res.status(200).json({
            success: true,
            response
        })

    } catch (error: any) {
        console.error('‚ùå Erreur Copilot:', error)

        // Erreur OpenAI sp√©cifique
        if (error?.error?.type === 'invalid_request_error') {
            return res.status(400).json({
                success: false,
                error: 'Requ√™te invalide vers OpenAI'
            })
        }

        if (error?.error?.code === 'insufficient_quota') {
            return res.status(503).json({
                success: false,
                error: 'Quota OpenAI d√©pass√©. R√©essayez plus tard.'
            })
        }

        return res.status(500).json({
            success: false,
            error: 'Erreur serveur lors de la g√©n√©ration de la r√©ponse'
        })
    }
}
